rm(list = ls())

# load packages
require(tidyverse)
require(sf)
require(osmdata)
require(factoextra)
require(cluster)


# k-means cluster analysis ####

### Here we DEFINITELY need to add an explanation of the k-means clustering algorithm, what it does (with scientific notation!!), and talk about hierarchical clustering too! Also talk about the random initialization of the algorithm with more authority in the lower section. 
# talk also about possible shortcomings: since it's about distance, it doesn't take into account any other factors, like the shape of a grouping of data. This doesnt matter an incredible amount for our purposes here however, given that we know there are more or less linear relationships between the variables, and that groups probably aren't so weird.
# 
# pop <- read.csv("C:/Users/moctar.aboubacar/Desktop/SAE_pop_bldg.csv")
# SAE <- merge(SAE, pop, by = intersect(names(SAE), names(pop))) 

# load the data
load("C:/Users/moctar.aboubacar/Desktop/R/2.clustering/data.RData")


#
# save(list = c("nepal", "province", "district", "SAE"),
#       file = "C:/Users/moctar.aboubacar/Desktop/R/2.clustering/data.RData")


# clean and subset data ####
# bad housing variable
SAE$bad.housing <- (SAE$house.bamboo + SAE$house.wood.planks + SAE$house.unbaked.bricks + SAE$house.mud..brick.and.stone)/(SAE$house.bamboo + SAE$house.unbaked.bricks + SAE$house.wood.planks + SAE$house.cement..brick.and.stone + SAE$house.mud..brick.and.stone + SAE$house.not.stated + SAE$house.others)

# osm queries
m <- matrix(c(79, 26, 88.5, 30.5), ncol = 2, byrow = TRUE)
row.names(m) <- c("x", "y")
names(m) <- c("min", "max")

q.school <- m %>% 
  opq(timeout = 100) %>% 
  add_osm_feature("amenity", "school") %>% 
  osmdata_sf()

schools <- q.school$osm_points

q.college <- m %>% 
  opq(timeout = 100) %>% 
  add_osm_feature("amenity", "college") %>% 
  osmdata_sf()

college <- q.college$osm_points

q.uni <- m %>% 
  opq(timeout = 100) %>% 
  add_osm_feature("amenity", "university") %>% 
  osmdata_sf()
uni <- q.uni$osm_points

q.bus <- m %>% 
  opq(timeout = 100) %>% 
  add_osm_feature("amenity", "bus_station") %>% 
  osmdata_sf()
bus <- q.bus$osm_points


# keep only points within Nepal
schools <- st_intersection(x = schools[1], y = nepal)
college <- st_intersection(x = college[1], y = nepal)
uni <- st_intersection(x = uni[1], y = nepal)
bus <- st_intersection(x = bus[1], y = nepal)

# counts of each point of interest by Palika
SAE$schools <- lengths(st_covers(SAE, schools))
SAE$colleges <- lengths(st_covers(SAE, college))
SAE$univ <- lengths(st_covers(SAE, uni))
SAE$bus <- lengths(st_covers(SAE, bus))

# combine college + universities
SAE$higher_edu <- SAE$colleges + SAE$univ


# calculate OSM counts per 1000 people
SAE <- SAE %>% 
  mutate(econ.per.cap = (banks + markets / Popn2019) * 1000,
         health.per.cap = (hospitals + clinics / Popn2019) * 1000,
         schools.per.cap = (schools / Popn2019) * 1000,
         higher_edu.per.cap = (higher_edu / Popn2019) * 1000,
         bus.per.cap = (bus / Popn2019) * 1000)



# visualize the data: (1) maps of the 4 POIs (like previous one) + (2) maps of the SAE variables (like previous one)


# create clustering df
# function to return column names
colname.fun <- function(df, cols){
  vec <- vector()
  for (i in cols){
    vec[i] <- which(colnames(df) == i)
  }
  return(vec)
}

# create key df w/o spatial geometries
SAE.clust.key <- SAE %>% 
  st_drop_geometry() %>% 
  filter(LU_Type != "National Park",
         LU_Type != "Hunting Reserve",
         LU_Type != "Wildlife Reserve",
         Avg_P0WB > 0)

clust.cols <- colname.fun(SAE.clust.key, cols = c("Avg_P0", "Avg_S2", "Avg_W2", "Avg_U2", "Avg_P0WB", "bad.housing", "schools.per.cap", "higher_edu.per.cap", "bus.per.cap"))

SAE.clust <- SAE.clust.key[,clust.cols]

# standardize data
SAE.clust <- scale(SAE.clust)

# save data
save(list = c("SAE","SAE.clust.key", "SAE.clust",
              "nepal", "province", "district", "schools", "college", "uni", "bus"),
     file = "C:/Users/moctar.aboubacar/Desktop/R/2.clustering/data.RData")



# examine within group sum of squares for different numbers of k (elbow plot)
# As the k-means algorithm has a 'random' initialization of centroids and iterative corrections, each run can produce slightly different results which could lead (possibly, but not incredibly likely) to the selection of a 'k' more heavily influenced by random chance (or overinterpretation of kinks in the elbow plot). Creating a function to gather means from 1000+ simulations of the algorithm gives a more generalized picture of what we can expect the progression of the within-group sum of squares to be as k grows. It ensures that we are not making a choice of k in any way informed by chance due to the randomness of the algorithm's starting position.

kmeans.sim <- function(df = SAE.clust, iterations = 1000, k = 10) {
  
  store <- matrix(nrow = iterations, ncol = k)
  
  for (i in c(1:iterations)){
    
    vec <- map_dbl(1:k, function(k){
      model.k <- kmeans(x = df, centers = k, nstart = 25)
      model.k$tot.withinss
    })
    
    store[i,] <-  vec
    
  }
  return(colMeans(store))
}

tot_withinss <- kmeans.sim()



# put data together and plot it out

scree_df <- data.frame(
  k = 1:10,
  tot_withinss = tot_withinss
)

scree.plot <- ggplot(scree_df, aes(x = k, y = tot_withinss))+
  geom_line(size = 1)+
  geom_point()+
  scale_x_continuous(breaks = 1:10)+
  labs(title = "Simulated K-means Scree Plot",
       subtitle = "Simulated within-cluster sum of squares for k = 1 through k = 20",
       x = "Number of clusters",
       y = "Total within-group sum of squares")

scree.plot # need to put this back into the reduced dataset

# This elbow plot is not incredibly satisfactory, but we would be looking to about between 5 and 9 clusters
fviz_nbclust(SAE.clust, kmeans, method = "wss") 


# silhouette analysis + PCA visualization

####################################################

fviz_nbclust(k.max = 10, SAE.clust, kmeans, method = "silhouette") # results are different here...but 5 is the best bet, parsimonious compared to 10, so given similar levels, we default to the most parsimonious/smallest and probably best interpretation given the data
# silhouette is about how close a point is to its own cluster vs to other clusters. A value of 0 means equally close to own cluster and to other clusters, and the cloer to 1 it gets the more each point is the 'rght fit' for the cluster it belongs to.


# with 5 clusters, we look at how they line up on the 2 first principal components (which catch a large part of the 'similar' variation in all cariables used to cluster). Here we note that 5 clusters have very little overlap (which is good), but that points are also rather close to one another.
k5 <- kmeans(SAE.clust, centers = 5, nstart = 25)
fviz_cluster(k5, geom = "point", data = SAE.clust)+
  ggtitle("k = 5")+
  theme_classic() # low to no overlap...but fundamentally similar things, the grouping is not crazy stark

####################################################

# k = 5 clustering
model.k5 <- kmeans(SAE.clust, centers = 5, nstart = 25)
SAE.clust.key$k_5 <- model.k5$cluster


# re-merge data and deal with missing cases
SAE <- merge(SAE, SAE.clust.key[c("HLCIT_CODE", "k_5")], all.x = TRUE)
SAE$k_5[is.na(SAE$k_5)] <- 6


# map of clusters k = 5
ggplot(SAE)+
  geom_sf(aes(fill = factor(k_5), size = NA))+
  labs(title = "K-means palika clustering (k = 5)",
       fill = "Classes")+
 scale_fill_manual(values = c("#eba53c", "#ac3d1f", "#207188", "#86aa6d", "purple", "grey20"),
                  labels = c("1", "2", "3", "4", "5", "NA"))+
  theme_void()



# boxplots by cluster
clust.cols2 <- colname.fun(SAE.clust.key, cols = c("Avg_P0", "Avg_S2", "Avg_W2", "Avg_U2", "Avg_P0WB", "bad.housing", "schools.per.cap", "higher_edu.per.cap", "bus.per.cap"))

SAE.clust.long.nut <- SAE %>% 
  gather(key = "key",
         value = "value",
         clust.cols2) %>% # variables to examine
  filter(k_5 != 6)

ggplot(SAE.clust.long.nut, 
       aes(x = factor(k_5), y = value, fill = factor(k_5)))+
  geom_boxplot(outlier.colour = "grey70", outlier.size = 1)+
  theme_bw()+
  scale_fill_manual(values = c("#eba53c", "#ac3d1f", "#207188", "#86aa6d", "purple"))+
  facet_wrap(scales = 'free_y', ~key)




# table by cluster
table.clust <- SAE %>%
  st_drop_geometry() %>% 
  group_by(k_5) %>%
  filter(k_5 != 6) %>% 
  summarize(Pov = mean(Avg_P0WB),
            Food_pov = mean(Avg_P0),
            Stunt = mean(Avg_S2),
            Wast = mean(Avg_W2),
            Undernour = mean(Avg_U2),
            Housing = mean(bad.housing),
            Schools = round(median(schools), 1),
            Bus_stop = median(bus))

print(table.clust)


# clustering by District instead of Palikas

# by-district clustering
#load("C:/Users/moctar.aboubacar/Desktop/R/2.clustering/data.RData")

# create key df w/o spatial geometries
SAE.dist <- SAE %>% 
  filter(LU_Type != "National Park",
         LU_Type != "Hunting Reserve",
         LU_Type != "Wildlife Reserve",
         Avg_P0WB > 0) %>% 
  group_by(DISTRICT) %>% 
  summarize(Pop = sum(Popn2019),
            Pov = mean(Avg_P0WB),
            Foodpov = mean(Avg_P0),
            Stunting = mean(Avg_S2),
            Wasting = mean(Avg_W2),
            Underweight = mean(Avg_U2),
            Housing = mean(bad.housing),
            school_cap = (sum(schools) / Pop) * 1000,
            college_cap = (sum(SAE$colleges + SAE$univ) / Pop) * 1000,
            bus_cap = (sum(bus) / Pop) * 1000) %>% 
  select(-Pop)


# create key df w/o geometries
dist.clust.key <- st_drop_geometry(SAE.dist)

dist.clust <- dist.clust.key[, -1]

dist.clust <- scale(dist.clust)


fviz_nbclust(dist.clust, kmeans, method = "wss") 
fviz_nbclust(k.max = 10, dist.clust, kmeans, method = "silhouette")
# 3, 5, and 6 clusters look ok. We choose 5 for comparison with previous analysis


dist.k5 <- kmeans(dist.clust, centers = 5, nstart = 25)
dist.clust.key$k_5 <- dist.k5$cluster


# re-merge data and deal with missing cases
SAE.dist <- merge(SAE.dist, dist.clust.key[c("DISTRICT", "k_5")], all.x = TRUE)

# map it out
ggplot(SAE.dist)+
  geom_sf(aes(fill = factor(k_5)))+
  labs(title = "K-means District clustering (k = 5)",
       fill = "Classes")+
  scale_fill_manual(values = c("#eba53c", "#ac3d1f", "#207188", "#86aa6d", "purple"),
                    labels = c("1", "2", "3", "4", "5"))+
  theme_void()


# boxplots by cluster
dist.cols <- colname.fun(SAE.dist, cols = c("Pov", "Foodpov", "Stunting", "Wasting", "Underweight", "Housing", "school_cap", "college_cap", "bus_cap"))

dist.clust.long.nut <- SAE.dist %>% 
  gather(key = "key",
         value = "value",
         dist.cols)# variables to examine

ggplot(dist.clust.long.nut, 
       aes(x = factor(k_5), y = value, fill = factor(k_5)))+
  geom_boxplot(outlier.colour = "grey70", outlier.size = 1)+
  theme_bw()+
  scale_fill_manual(values = c("#eba53c", "#ac3d1f", "#207188", "#86aa6d", "purple"))+
  facet_wrap(scales = 'free_y', ~key)




k5.dist <- kmeans(dist.clust, centers = 5, nstart = 25)
fviz_cluster(k5.dist, geom = "point", data = dist.clust)+
  ggtitle("k = 5")+
  theme_classic()

# less overlap in these graphs too now
ggplot(SAE.dist, aes(x = Wasting, y = Foodpov, color = factor(k_5)))+
  geom_point()+
  








# discussion
# results are not about targeting priority, but give can give us a more nuanced understanding of what different needs governments and communities have. We know about the west having high stunting and the Terai having high wasting, but what about Jhapa!! The needs of the Province 1 palikas are a little different.
# there is a data problem here (insofar as it concerns the use of OSM points) because we are assuming that there is no missing data; this is surely not the case, as we can't assume that the OSM database is complete. It will overcount urban areas and accentuate differences with rural areas (seen from the skewed boxplots)-- also mapping efforts in certain areas (for example, post earthquake Province 3- Gorkha areas) may make those areas look better or more serviced compared to other areas that were not covered. We talked about the Humla problem in the previous post, but looks like it was properly classified here

# Ultimately, more rigorous approaches to clustering should : (1) not consider OSM data and (2) consider other data as it is available, including district-level data, HMIS etc.